{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# armamento del Data Frame proveniente de Scikit-Learn\n",
    "\n",
    "data = datasets.fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(data[\"data\"],columns=data[\"feature_names\"])\n",
    "df[\"MedHouseVal\"] = data[\"target\"]\n",
    "df[\"MedHouseVal\"] = df[\"MedHouseVal\"] * 100000\n",
    "\n",
    "df_model = df.copy() # generando una copia, asi no afectamos al Data Frame original\n",
    "\n",
    "def remove_outliers(df,columns):\n",
    "    for c in columns:\n",
    "        df[c] = df[c].mask(zscore(df[c]).abs() > 3, np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# removiendo valores atípicos que afecten el desempeño del modelo\n",
    "df_model = remove_outliers(df_model, df.columns)\n",
    "df_model.dropna(inplace=True)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search CV(Cross-Validation)\n",
    "Es una función utilizada en la búsqueda de los hiperparámetros de un modelo de aprendizaje automático. Su proceso se basa en crear un cuadrícula de todos los posibles valores, se realizan diferentes combinaciones de hiperparámetros para generar una variabilidad de modelos y se seleccionrán aleatoriamente algunos de ellos sobre los cuales se pondrán a prueba mediante una técnica denominada Cross-Validation, que consiste en las particiones de conjuntos de datos en n cantidades iguales y en las que una será utlizada para evualar el rendimiento del modelo generado, repitiendo el proceso n veces. Por último, la función almacenará no solo las puntuaciones obtenidas sino también los parámetros cuyo desempeño fue mayor al resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[df.columns[:-1]],\n",
    "                                                    df[\"MedHouseVal\"],\n",
    "                                                    test_size=0.25)\n",
    "\n",
    "# ajuste de hiperparámetros utilizando RandomizedSearchCV()\n",
    "\n",
    "xgbr_test = XGBRegressor()           \n",
    "\n",
    "turned_parameters = {\n",
    "    \"n_estimators\":[100,200,300,400],\n",
    "    \"subsample\":[0.7,0.75,0.8,0.85],\n",
    "    \"max_depth\":[3,4,5,6],\n",
    "    \"learning_rate\":[0.2,0.3,0.4,0.5],\n",
    "    \"min_child_weight\":[2,3,4,5],\n",
    "    \"gamma\":[0,1,2,3]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(xgbr_test, turned_parameters,cv=5)\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "print(f'''n_estimators(número de modelos creados): {random_search.best_params_[\"n_estimators\"]}\n",
    "subsample(tamaño de muestra requerida): {random_search.best_params_[\"subsample\"]}\n",
    "max_depth(máxima profundidad de cada árbol): {random_search.best_params_[\"max_depth\"]}\n",
    "learning_rate(tasa de aprendizaje, evita el sobreajuste): {random_search.best_params_[\"learning_rate\"]}\n",
    "min_child_weight(suma mínima de peso de instancia necesaria en un nodo): {random_search.best_params_[\"min_child_weight\"]}\n",
    "gamma(cuanto mayor sea, más conservador será el modelo): {random_search.best_params_[\"gamma\"]}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "En el ejemplo anterior habiamos mencionado y ejecutado la técnica de Bagging, donde generábamos distintos metaestimadores entrenados de forma independiente y se tomaban en cuenta los resultados de cada uno. En este caso, cada metaestimador capacitará y solucionará los errores del siguiente potenciando cada vez más la precisión final, es decir que su escalabilidad es vertical en lugar de horizontal. Requieren de una hiperparametrización compleja y una tendencia, si no realiza correctamente lo anterior mencionado, al sobreajuste, es decir, modelos con una precisión alta sobre un grupos de datos en particular pero con la poca adaptación para resolver nuevas situaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asignando valores de los modelos que obtuvieron los mejores resultados\n",
    "\n",
    "xgbr = XGBRegressor(n_estimators = random_search.best_params_[\"n_estimators\"],\n",
    "                    subsample = random_search.best_params_[\"subsample\"],\n",
    "                    max_depth = random_search.best_params_[\"max_depth\"],\n",
    "                    learning_rate = random_search.best_params_[\"learning_rate\"],\n",
    "                    min_child_weight = random_search.best_params_[\"min_child_weight\"],\n",
    "                    gamma = random_search.best_params_[\"gamma\"])\n",
    "\n",
    "xgbr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas de regresión\n",
    "Tienen mucha similitud con las medidas de variabilidad y pueden plantearse analogías entre estas, con la diferencia de que emplean los residuos del modelo para obtener los valores:\n",
    "\n",
    "Error absoluto medio(MAE): similar a la desviacíon media absoluta, es la división por el número de muestra de la sumatoria de los residuos absolutos.\n",
    "\n",
    "Error cuadrático medio(MSE): similar a la varianza, es la división por el número de muestra de la sumatoria de los residuos al cuadrado.\n",
    "\n",
    "Raíz cuadrada del error cuadrático medio(RMSE): similar a la desviación éstandar, se utiliza para convertir a la unidad base de los valores al error cuadrático medio.\n",
    "\n",
    "Coeficiente de determinación(R2 Score): valor de 0 a 1 que evalúa la bondad de ajuste de nuestro modelo a los datos, pero solo utilizada en modelos lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regresión\n",
    "predicts = xgbr.predict(x_test)\n",
    "\n",
    "# validación con métricas de rendimiento\n",
    "metrics = f\"MAE: {round(mean_absolute_error(y_test,predicts))} | MSE: {round(mean_squared_error(predicts,y_test))} | RMSE: {round(root_mean_squared_error(predicts,y_test))}\"\n",
    "scatter = go.Figure()\n",
    "scatter.add_trace(go.Scatter(x=predicts,y=y_test,mode=\"markers\",marker_color=\"red\"))\n",
    "scatter.update_layout(title=metrics)\n",
    "scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dashboard que funciona como mapa descriptivo de las viviendas de California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div(id=\"body\",className=\"e5_body\",children=[\n",
    "    html.H1(\"Viviendas en California \",id=\"title\",className=\"e5_title\"),\n",
    "        html.Div(id=\"div\",className=\"e5_div\",children=[\n",
    "            dcc.Dropdown(id=\"dropdown\",className=\"e5_dropdown\",\n",
    "                        options = [\n",
    "                            {\"label\":\"Valor de precio\",\"value\":\"MedHouseVal\"},\n",
    "                            {\"label\":\"Ingreso medio\",\"value\":\"MedInc\"},\n",
    "                            {\"label\":\"Edad media\",\"value\":\"HouseAge\"},\n",
    "                            {\"label\":\"Promedio de habitaciones\",\"value\":\"AveRooms\"},\n",
    "                            {\"label\":\"Promedio de dormitorios\",\"value\":\"AveBedrms\"},\n",
    "                            {\"label\":\"Población\",\"value\":\"Population\"},\n",
    "                            {\"label\":\"Promedio de ocupación\",\"value\":\"AveOccuption\"}\n",
    "                        ],\n",
    "                        value=\"MedHouseVal\",\n",
    "                        multi=False,\n",
    "                        clearable=False)]),\n",
    "            dcc.Graph(id=\"graph\",className=\"e5_graph\",figure={})\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(component_id=\"graph\",component_property=\"figure\"),\n",
    "    [Input(component_id=\"dropdown\",component_property=\"value\")]\n",
    ")\n",
    "\n",
    "def update_graph(slct_var):\n",
    "    \n",
    "    scatter_map = px.scatter(df,x=\"Longitude\",y=\"Latitude\",color=slct_var)\n",
    "    \n",
    "    return scatter_map\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aprendizaje no supervisado\n",
    "las observaciones no tienen una respuesta asociada que guíe el aprendizaje, uno de sus algoritmos es Kmeans y genera. mediante un proceso iterativo, sus propias etiquetas determinando grupos de datos asociables en función de sus acercamientos estadísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "inertias = []\n",
    "\n",
    "for c in range(3,12):\n",
    "    kmeans = KMeans(n_clusters=c).fit(df[\"MedHouseVal\"].values.reshape((-1,1)))\n",
    "    clusters.append(c)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "kmeans = KMeans(n_clusters=5).fit(df[\"MedHouseVal\"].values.reshape((-1,1)))\n",
    "inertia = kmeans.inertia_\n",
    "\n",
    "plt.plot(clusters, inertias, marker=\"o\")\n",
    "plt.text(int(str(kmeans)[-2]) - 0.1, inertia, \"Valor del codo\")\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método del codo\n",
    "El modelo de clustering requiere de un hiperparámetro que es la cantidad de centroides o K-means, estos serán dispersados e irán apropiandose de los datos más cercanos hasta convertirse en las medias de los grupos que formaron(nuevamente, mediante pruebas iterativas), el método del codo se utiliza a la hora de designar este valor, dónde el objetivo es visualizar la menor cantidad de centroides y la menor inercia(alejamiento entre los miembros de clusters y su centroide). También existen métricas especializadas en evaluar esta clase de modelos, Coeficiente de silueta e Índice Davies-Bouldin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5).fit(df[\"MedHouseVal\"].values.reshape((-1,1)))\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "df[\"clusters\"] = clusters\n",
    "\n",
    "range_values = np.array([])\n",
    "\n",
    "for c in df[\"clusters\"].sort_values().unique():\n",
    "    cluster = df.loc[df[\"clusters\"] == c,[\"clusters\",\"MedHouseVal\"]]\n",
    "    max_value = str(cluster[\"MedHouseVal\"].max())\n",
    "    min_value = str(cluster[\"MedHouseVal\"].min())\n",
    "    range_values = np.append(range_values,min_value)\n",
    "    range_values = np.append(range_values,max_value)\n",
    "    \n",
    "range_values = range_values.reshape((-1,2))\n",
    "    \n",
    "df[\"clusters\"] = df[\"clusters\"].replace(\n",
    "    {\n",
    "        0:f\"0 ({range_values[0,0][:8]}$-{range_values[0,1][:8]}$)\",\n",
    "        1:f\"1 ({range_values[1,0][:8]}$-{range_values[1,1][:8]}$)\",\n",
    "        2:f\"2 ({range_values[2,0][:8]}$-{range_values[2,1][:8]}$)\",\n",
    "        3:f\"3 ({range_values[3,0][:8]}$-{range_values[3,1][:8]}$)\",\n",
    "        4:f\"4 ({range_values[4,0][:8]}$-{range_values[4,1][:8]}$)\"\n",
    "    })\n",
    "\n",
    "\n",
    "fig = px.scatter(df,x=\"Longitude\",y=\"Latitude\",color=\"clusters\")\n",
    "fig.update_layout(title=\"Range of Houses'values\")\n",
    "fig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análisis de clusters\n",
    "Ya tenemos una idea clara en cuanto a los rangos de precios que las viviendas suelen frecuentar y como su ubicación influye, por ejemplo: vemos un evindente aumento en las viviendas más cercanas a la costa de California, esto ubicando los cluster donde dentro de estos se encuentran los valores más caros siendo (296.800-414.100) y (414.300-500.000), también, observamos que los precios suelen valer entre 15.000 y 127.800 gracias al cluster que guarda esos valores y siendo el más numeroso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
